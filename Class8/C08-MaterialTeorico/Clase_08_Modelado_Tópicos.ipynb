{"cells":[{"cell_type":"markdown","metadata":{"id":"YqE21kC2Ardh"},"source":["# Modelado de Tópicos\n","\n","En este notebook presentaremos una técnica de aprendizaje no supervisado dentro del campo de procesamiento de lenguaje natural llamado Modelado de Tópicos (Topic Modeling). Esta técnica se busca en construir/identificar temas en base a las distribuciones de las palabras en un conjunto de documentos."]},{"cell_type":"markdown","metadata":{"id":"BjguYEhfDNvk"},"source":["## Conjunto de datos\n","\n","El conjunto de datos que trataremos en el presente notebook será un conjunto de plots (tramas) de aproximadamente 35,000 películas de Wikipedia.\n","\n","Referencia: https://www.kaggle.com/jrobischon/wikipedia-movie-plots"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"cpUHvV_CDO0K"},"outputs":[],"source":["# Importamos pandas\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gYu9btLdDje6"},"outputs":[{"data":{"text/plain":["(34886, 8)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Cargamos el dataset a pandas\n","df = pd.read_csv(\"wiki_movie_plots_deduped.csv\")\n","# Dimensiones del dataframe\n","df.shape"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"bJPqVsyHDp7h"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Release Year</th>\n","      <th>Title</th>\n","      <th>Origin/Ethnicity</th>\n","      <th>Director</th>\n","      <th>Cast</th>\n","      <th>Genre</th>\n","      <th>Wiki Page</th>\n","      <th>Plot</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>21057</th>\n","      <td>2009</td>\n","      <td>Triangle</td>\n","      <td>British</td>\n","      <td>Christopher Smith</td>\n","      <td>Melissa George, Liam Hemsworth, Rachael Carpani</td>\n","      <td>horror</td>\n","      <td>https://en.wikipedia.org/wiki/Triangle_(2009_B...</td>\n","      <td>While preparing to take her autistic son Tommy...</td>\n","    </tr>\n","    <tr>\n","      <th>26406</th>\n","      <td>2006</td>\n","      <td>Taxi No. 9211</td>\n","      <td>Bollywood</td>\n","      <td>Unknown</td>\n","      <td>John Abraham, Nana Patekar, Sameera Reddy, Son...</td>\n","      <td>social, thriller</td>\n","      <td>https://en.wikipedia.org/wiki/Taxi_No._9211</td>\n","      <td>Taxi No. 9 2 11 focuses on Raghav Shastri (Nan...</td>\n","    </tr>\n","    <tr>\n","      <th>23937</th>\n","      <td>2003</td>\n","      <td>Bhalo Theko</td>\n","      <td>Bengali</td>\n","      <td>Goutam Halder</td>\n","      <td>Soumitra Chatterjee, Vidya Balan</td>\n","      <td>unknown</td>\n","      <td>https://en.wikipedia.org/wiki/Bhalo_Theko</td>\n","      <td>The film is set in Acharya Jagadish Chandra Bo...</td>\n","    </tr>\n","    <tr>\n","      <th>19343</th>\n","      <td>1957</td>\n","      <td>Sea Wife</td>\n","      <td>British</td>\n","      <td>Bob McNaught</td>\n","      <td>Joan Collins, Richard Burton</td>\n","      <td>thriller</td>\n","      <td>https://en.wikipedia.org/wiki/Sea_Wife</td>\n","      <td>Michael Cannon (Richard Burton) returns to Lon...</td>\n","    </tr>\n","    <tr>\n","      <th>29572</th>\n","      <td>1978</td>\n","      <td>Thappu Thalangal</td>\n","      <td>Tamil</td>\n","      <td>K. Balachander</td>\n","      <td>Rajinikanth, Saritha</td>\n","      <td>unknown</td>\n","      <td>https://en.wikipedia.org/wiki/Thappu_Thalangal</td>\n","      <td>Devu, a local thug whose weapon of choice is a...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Release Year             Title Origin/Ethnicity           Director  \\\n","21057          2009          Triangle          British  Christopher Smith   \n","26406          2006     Taxi No. 9211        Bollywood            Unknown   \n","23937          2003       Bhalo Theko          Bengali      Goutam Halder   \n","19343          1957          Sea Wife          British       Bob McNaught   \n","29572          1978  Thappu Thalangal            Tamil     K. Balachander   \n","\n","                                                    Cast             Genre  \\\n","21057    Melissa George, Liam Hemsworth, Rachael Carpani            horror   \n","26406  John Abraham, Nana Patekar, Sameera Reddy, Son...  social, thriller   \n","23937                   Soumitra Chatterjee, Vidya Balan           unknown   \n","19343                       Joan Collins, Richard Burton          thriller   \n","29572                               Rajinikanth, Saritha           unknown   \n","\n","                                               Wiki Page  \\\n","21057  https://en.wikipedia.org/wiki/Triangle_(2009_B...   \n","26406        https://en.wikipedia.org/wiki/Taxi_No._9211   \n","23937          https://en.wikipedia.org/wiki/Bhalo_Theko   \n","19343             https://en.wikipedia.org/wiki/Sea_Wife   \n","29572     https://en.wikipedia.org/wiki/Thappu_Thalangal   \n","\n","                                                    Plot  \n","21057  While preparing to take her autistic son Tommy...  \n","26406  Taxi No. 9 2 11 focuses on Raghav Shastri (Nan...  \n","23937  The film is set in Acharya Jagadish Chandra Bo...  \n","19343  Michael Cannon (Richard Burton) returns to Lon...  \n","29572  Devu, a local thug whose weapon of choice is a...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YPS_7LDJU6ua"},"outputs":[],"source":["# Información de los géneros\n","df[\"Genre\"].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"8xuA9-lnFDIU"},"source":["## Limpieza de datos\n","\n","Haremos una pequeña limpieza de datos. Para esta ocasión nos ayudaremos de la librería spaCy. Filtraremos los \"plots\" que estén compuesto por palabras que contengan exclusivamente letras, que no sean \"stopwords\" y que no sean nombres propios. Luego obtendremos el lema de la palabra."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJaaH5osTV5r"},"outputs":[],"source":["# Importamos la librería spaCy\n","import spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jC11B41wS2a4"},"outputs":[],"source":["# Descargamos los paquetes\n","!python -m spacy download en_core_web_md"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9GRltS4Rnzw"},"outputs":[],"source":["# Cargamos los modelos\n","nlp = spacy.load(\"en_core_web_md\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5D60rYzFWQ2H"},"outputs":[],"source":["# Ahora definiremos nuestra función que nos ayudará a limpiar nuestros datos\n","def preprocess_text(text):\n","    tokens = []\n","    doc = nlp(text)\n","\n","    for token in doc:\n","        if ((token.is_alpha) and (not token.is_stop) and (token.pos_ != \"PROPN\")):\n","            tokens.append(token.lemma_)\n","\n","    preprocessed_text = ' '.join(tokens)\n","\n","    return preprocessed_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yguf65EkwYGi"},"outputs":[],"source":["# Utilizaremos esta librería que nos ayudará a \"estimar\" los tiempos de ejecución de algunas\n","# instrucciones\n","from tqdm import tqdm\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3uRRv4FKc40"},"outputs":[],"source":["# Aplicamos la función de preprocesamiento a cada \"Plot\" del dataframe\n","# Esta tarea puede demorar varios minutos (45 min)\n","df[\"plot_preprocessed\"] = df[\"Plot\"].progress_apply(preprocess_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2vddUfqK-9w"},"outputs":[],"source":["df.sample(5)[[\"Plot\", \"plot_preprocessed\"]]"]},{"cell_type":"markdown","metadata":{"id":"cB0bd8c9FjMY"},"source":["## Exploración de los datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4-y-wvQc9Nh"},"outputs":[],"source":["# Importamos algunas librerías que nos ayudará a explorar los datos\n","import nltk\n","from nltk import FreqDist\n","from nltk import word_tokenize\n","\n","nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJZAoXo3IJ40"},"outputs":[],"source":["# Obtenemos la distribución de los tokens\n","tokens_distribution = FreqDist(word_tokenize(' '.join(df[\"plot_preprocessed\"])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WDEHuRFIWE0"},"outputs":[],"source":["# Los tokens más comunes\n","tokens_distribution.most_common(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2g27XnuddFY"},"outputs":[],"source":["# Número de tokens\n","tokens_distribution.N()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-LzmJ3yIb2V"},"outputs":[],"source":["# Tamaño del vocabularios\n","len(tokens_distribution)"]},{"cell_type":"markdown","metadata":{"id":"5l9tXyhqJIQL"},"source":["## Latent Dirichlet Allocation (LDA)\n","\n","Es un modelo estadístico generativo que se utiliza como una técnica de modelado de temas que puede clasificar el texto de un documento en un tema en particular.\n","\n","Este modelo tiene una implementación en Scikit-Learn.\n","\n","Enlace: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jLUBSbkk_84"},"outputs":[],"source":["# Importamos CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYSkiqsQItq7"},"outputs":[],"source":["# Vectorizaremos los documentos, haremos un \"Bag of Words\" (BoW)\n","# min_df: ignora a los términos que aparecen en menos de un número de documentos\n","bow = CountVectorizer(min_df=10).fit(df[\"plot_preprocessed\"])\n","plot_bow = bow.transform(df[\"plot_preprocessed\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KX12o5ZUj-9U"},"outputs":[],"source":["# Dimensiones de nuestros BoW\n","plot_bow.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNcsxuK6pOaC"},"outputs":[],"source":["# Importamos la implementación LDA de Scikit-Learn\n","from sklearn.decomposition import LatentDirichletAllocation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNICBA29Jac8"},"outputs":[],"source":["# Instanciamos nuestro LDA\n","# Parámetros:\n","#   n_components: número de tópicos/temas\n","#   max_iter: el número máximo de \"pasos\" sobre la data de entrenamiento (épocas)\n","#   learning_method:\n","#       batch: Método de Bayes variacional por batches. Usar todos los datos de entrenamiento en cada actualización.\n","#       online: Método de Bayes variacional en línea. Usa mini-batches de datos de entrenamiento en cada actualización.\n","#   batch_size: número de documentos a utilizar en cada actualización/iteración\n","lda = LatentDirichletAllocation(n_components=10, max_iter=15, learning_method=\"online\", batch_size=256, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUyPC76_Jarf"},"outputs":[],"source":["%%time\n","# Alimentamos nuestro modelo LDA con la data de \"plots\"\n","lda.fit(plot_bow)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJVq83wHKxKA"},"outputs":[],"source":["# Componentes\n","lda.components_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDGV97J2Kzuz"},"outputs":[],"source":["# Número de componentes, número de tópicos\n","len(lda.components_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EE7OF8ujK5TM"},"outputs":[],"source":["# Número de \"variables\" por cada componente\n","len(lda.components_[0])"]},{"cell_type":"markdown","metadata":{"id":"DplX0w3K2YAH"},"source":["¿Cómo saber las palabras dentro de cada tópico?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0aotmvULBNo"},"outputs":[],"source":["for id, topic in enumerate(lda.components_):\n","    # Ordenamos los tópicos de menor a mayor y obtenemos las 10 últimas palabras\n","    words = [bow.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]\n","    print(\"Topic {}: {}\".format(id, ' '.join(words)))"]},{"cell_type":"markdown","metadata":{"id":"oIuPSwTG4GxT"},"source":["¿Cómo saber los tópicos por un documento en específico?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpfZGgN5ID4d"},"outputs":[],"source":["# Importamos algunas librerías que nos ayudará a la visualización de los datos\n","import seaborn as sns\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GlXZgSZBrRf"},"outputs":[],"source":["# Película a buscar\n","movie = df.loc[df[\"Title\"] == \"Coco\"]\n","movie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udgbhF7hMnrx"},"outputs":[],"source":["# Pesos por cada tópico (por ciento)\n","movie_topics = lda.transform(plot_bow[movie.index]).flatten() * 100\n","movie_topics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WPgQ5axeTByU"},"outputs":[],"source":["# Visualizamos los tópicos para la película \"Coco\"\n","sns.barplot(x=np.arange(len(movie_topics)), y=movie_topics)"]},{"cell_type":"markdown","metadata":{"id":"JKU2HeNnlpJ8"},"source":["¿Cómo saber a que tópico pertenece cada documento del dataset?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MF9QZBHNK3-"},"outputs":[],"source":["# Para cada plot, obtenemos el tópico que tiene mayor porcentaje\n","topics = [lda.transform(plot).argsort()[0][-1] for plot in plot_bow]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IyEr9z3NWzT"},"outputs":[],"source":["# Visualizamos los documentos por tópicos\n","sns.countplot(x=topics)"]},{"cell_type":"markdown","metadata":{"id":"8y_ZaN-P8EGo"},"source":["## Evaluación del modelo\n","\n","Podemos usar la puntuación de coherencia en el modelado de tópicos para medir qué tan interpretables son los temas para los humanos. En este caso, los temas se representan como las primeras N palabras con mayor probabilidad de pertenecer a ese tema en particular. En otras palabras, el puntaje de coherencia mide qué tan similares son estas palabras entre sí."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Qx1-0NdBwR3"},"outputs":[],"source":["# Instalamos la librería que nos ayudará a computar la métrica de coherencia\n","!pip install tmtoolkit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N19O_Wkj8Gjj"},"outputs":[],"source":["# Importamos la métrica\n","from tmtoolkit.topicmod.evaluate import metric_coherence_gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPUI-hsNK4aL"},"outputs":[],"source":["# Calculamos la métrica para los tópicos\n","# Parámatros:\n","#     measure: 'c_v' coherence value\n","#     top_n: número de las palabras más probables por tópico\n","#     topic_word_distrib: distribución de palabras por tópico, dimensiones KxM, donde K es el número de tópicos y\n","#                           M es el tamaño de vocabulario\n","#     vocab: array o lista del vocabulario\n","#     texts: lista de los documentos tokenizados\n","metric_coherence_gensim(measure=\"c_v\", top_n=10, topic_word_distrib=lda.components_,\n","                        vocab=np.array([key for key in bow.vocabulary_.keys()]),\n","                        texts=[word_tokenize(plot) for plot in df[\"plot_preprocessed\"]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4U4m5pBNOtqN"},"outputs":[],"source":["for id, topic in enumerate(lda.components_):\n","    # Ordenamos los tópicos de menor a mayor y obtenemos las 10 últimas palabras\n","    words = [bow.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]\n","    print(\"Topic {}: {}\".format(id, ' '.join(words)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfcNKHUxyMQY"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
