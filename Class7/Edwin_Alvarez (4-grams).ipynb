{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drYta2F_msMu"
      },
      "source": [
        "# Modelo Estadístico N-Gram\n",
        "\n",
        "Es un modelo probabilístico que se entrena a través de un corpus. Este modelo es útil en muchas aplicaciones de procesamiento de lenguaje natural como reconocimiento de voz, traducción automática, predicción de texto, etc.\n",
        "\n",
        "Básicamente, en un modelo \"n-gram\" se construye en base a la frecuencia que ocurre una secuencia de palabras en un texto para luego predecir la siguientes palabras.\n",
        "\n",
        "Lo que haremos es crear un modelo \"n-gram\" sobre un corpus y luego, en base a dos palabras que le daremos al modelo, este intentará de predecir las siguientes palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "bpbfO9rpnL1Q"
      },
      "outputs": [],
      "source": [
        "# Importamos la librería nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltqKNHXGB3ep"
      },
      "source": [
        "El dataset que trabajaremos en esta ocasión será el corpus de \"Reuters\". Este contiene 10,788 noticias compuesto por un total de 1.3 millones de palabras. Estas noticias han sido clasificadas dentro de 90 categorías y agrupadas en dos conjuntos de datos: entrenamiento y pruebas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "L7sz77d-nHyI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /home/win7/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Descargamos el corpus \"Reuters\"\n",
        "nltk.download(\"reuters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "kxtmcZEZeUmU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/win7/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Y también descargaremos el paquete \"punkt\" que nos ayudará a tokenizar los textos\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "K1vPvDjymsMu"
      },
      "outputs": [],
      "source": [
        "# Importamos la librería de \"reuters\"\n",
        "from nltk.corpus import reuters\n",
        "# También la de \"trigrams\"\n",
        "from nltk import trigrams\n",
        "from nltk.util import ngrams\n",
        "# Y finalmente algunas funciones para manejo de diccionarios y contadores\n",
        "from collections import Counter, defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "Zs8IFFmlLySn"
      },
      "outputs": [],
      "source": [
        "# Creamos una variable, en este caso diccionario de diccionarios que nos ayudará a guardar el conteo de las ocurrencias de cadenas de palabras.\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "DKc-8nAYg85p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Now',\n",
              " 'it',\n",
              " \"'\",\n",
              " 's',\n",
              " 'largely',\n",
              " 'out',\n",
              " 'of',\n",
              " 'their',\n",
              " 'hands',\n",
              " ',\"',\n",
              " 'said',\n",
              " 'Kleinwort',\n",
              " 'Benson',\n",
              " 'Ltd',\n",
              " 'financial',\n",
              " 'analyst',\n",
              " 'Simon',\n",
              " 'Smithson',\n",
              " '.']"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ejemplo de una oración del corpus de \"reuters\"\n",
        "reuters.sents()[100]\n",
        "# model[\"Now\", \"it\"][\"'\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "R7Y40-T6RI8r"
      },
      "outputs": [],
      "source": [
        "# Crearemos una matriz de coocurrencia a través del diccionario.\n",
        "\n",
        "# Para cada oración del corpus \"reuters\" ...\n",
        "for sentence in reuters.sents():\n",
        "    # Para cada trigrama de una oración ...\n",
        "    # pad_xxx significa que se agregará un token de inicio o de fin a la oración\n",
        "    for w1, w2, w3, w4 in ngrams(sentence, 4, pad_right=True, pad_left=True):\n",
        "        # Calculamos la frecuencia con la que ocurre cada combinación de trigramas en el conjunto de datos\n",
        "        # Hay que verlo como si en la matriz, las filas son la secuencia w1, w2 y las columnas w3 tienen la palabra a \"predecir\"\n",
        "        model[(w1, w2, w3)][w4] += 1 # pseudo probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "jRXjiF6ZjWRZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('term', 'Tokyo', \"'\"),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>, {'s': 1})),\n",
              " (('Tokyo', \"'\", 's'),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
              "              {'loss': 1,\n",
              "               'latest': 1,\n",
              "               'package': 1,\n",
              "               'export': 1,\n",
              "               'partners': 1,\n",
              "               'foreign': 1,\n",
              "               'failure': 4,\n",
              "               'alleged': 4,\n",
              "               'stock': 1,\n",
              "               'prime': 1,\n",
              "               'Shin': 1,\n",
              "               'request': 1,\n",
              "               'economic': 1})),\n",
              " ((\"'\", 's', 'loss'),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
              "              {'might': 1, '-': 1, 'to': 1, 'of': 4})),\n",
              " (('s', 'loss', 'might'),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>, {'be': 1})),\n",
              " (('loss', 'might', 'be'),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>, {'their': 1}))]"
            ]
          },
          "execution_count": 187,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Volvamos a ver los items de nuestro objeto\n",
        "list(model.items())[105:110]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "fKPTUimWVMxN"
      },
      "outputs": [],
      "source": [
        "# Ahora recorremos cada secuencia w1, w2 o fila del modelo\n",
        "for w1_w2 in model:\n",
        "    # Y para cada secuencia w1, w2 contamos la cantidad de veces que esa secuencia se encuentra presente en el modelo\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    # Y ese valor lo usamos para calcular las probabilidades de una palabra w3, dada las dos anteriores palabras\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "jbsk0zi31JNZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('term', 'Tokyo', \"'\"),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>, {'s': 1.0})),\n",
              " (('Tokyo', \"'\", 's'),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
              "              {'loss': 0.05263157894736842,\n",
              "               'latest': 0.05263157894736842,\n",
              "               'package': 0.05263157894736842,\n",
              "               'export': 0.05263157894736842,\n",
              "               'partners': 0.05263157894736842,\n",
              "               'foreign': 0.05263157894736842,\n",
              "               'failure': 0.21052631578947367,\n",
              "               'alleged': 0.21052631578947367,\n",
              "               'stock': 0.05263157894736842,\n",
              "               'prime': 0.05263157894736842,\n",
              "               'Shin': 0.05263157894736842,\n",
              "               'request': 0.05263157894736842,\n",
              "               'economic': 0.05263157894736842})),\n",
              " ((\"'\", 's', 'loss'),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
              "              {'might': 0.14285714285714285,\n",
              "               '-': 0.14285714285714285,\n",
              "               'to': 0.14285714285714285,\n",
              "               'of': 0.5714285714285714})),\n",
              " (('s', 'loss', 'might'),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>, {'be': 1.0})),\n",
              " (('loss', 'might', 'be'),\n",
              "  defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
              "              {'their': 1.0}))]"
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Volvamos a ver los items de nuestro objeto\n",
        "list(model.items())[105:110]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "pJS104NUmEOy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\"',\n",
              " 'The',\n",
              " 'government',\n",
              " ',',\n",
              " 'however',\n",
              " ',',\n",
              " 'does',\n",
              " 'not',\n",
              " 'want',\n",
              " 'to',\n",
              " 'accelerate',\n",
              " 'reducing',\n",
              " 'the',\n",
              " 'debt',\n",
              " 'by',\n",
              " 'making',\n",
              " 'an',\n",
              " 'excessive',\n",
              " 'trade',\n",
              " 'surplus',\n",
              " ',\"',\n",
              " 'he',\n",
              " 'said',\n",
              " '.']"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Otra oración ejemplo del corpus \"reuters\"\n",
        "reuters.sents()[200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdesd9bvmsMx",
        "outputId": "0b013f9a-0160-480e-ea7f-617b663a93da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\" The government , however , does not want to accelerate reducing the debt by making an excessive trade surplus ,\" he said .\n"
          ]
        }
      ],
      "source": [
        "# Lo imprimimos de otra forma\n",
        "print(' '.join(reuters.sents()[200]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "3n8hZR6fp7jA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('to', 0.6470588235294118),\n",
              " ('a', 0.11764705882352941),\n",
              " ('for', 0.058823529411764705),\n",
              " ('area', 0.058823529411764705),\n",
              " ('sterling', 0.058823529411764705),\n",
              " ('them', 0.058823529411764705)]"
            ]
          },
          "execution_count": 192,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Probamos con una secuencia de dos palabras que se extrajo de la oración anterior\n",
        "sorted(dict(model[(\"does\", \"not\", \"want\")]).items(), key=lambda x:x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "Afw7HnOGjHqL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('fair', 0.25), ('very', 0.25), ('relatively', 0.25), ('based', 0.25)]"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Otro ejemplo para la secuencia \"the price\", podemos ver todas las posibles palabras que pueden venir a continuación de esta secuencia y con sus\n",
        "# respectivas probabilides de aparición\n",
        "sorted(dict(model[(\"the\", \"price\", \"is\")]).items(), key=lambda x:x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk8qh-DmmsMz",
        "outputId": "20c36136-29ad-48b0-e3e0-6e66fbcabcd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 194,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Otro ejemplo con una secuencia que intencionalmente está mal escrita en inglés \"the today\"\n",
        "sorted(dict(model[(\"the\", \"today\", \"was\")]).items(), key=lambda x:x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "nwR4A7jNsDah"
      },
      "outputs": [],
      "source": [
        "# Importaremos la librería \"random\"\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "C7xTaC3hsJks"
      },
      "outputs": [],
      "source": [
        "# Elegiremos una secuencia de 2 palabras con la cuál se comenzará a crear una nueva oración\n",
        "text = [\"the\", \"price\", \"is\"]\n",
        "# Y también declararemos una variable que nos ayudará a determinar cuándo se acabó la oración\n",
        "sentence_finished = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['fair', 'very', 'relatively', 'based'])"
            ]
          },
          "execution_count": 197,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model[tuple(text[-3:])].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "Mxsy-UilmsM1"
      },
      "outputs": [],
      "source": [
        "# Iteramos mientras la oración no haya terminado\n",
        "while not sentence_finished:\n",
        "    # generamos un número aleatorio del 0 al 1 que será nuestro threshold\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        "    # iteramos sobre el conjunto de posibles palabras que pueden venir luego de una secuencia de otras dos palabras (las ultimas 2 del texto)\n",
        "    for word in model[tuple(text[-3:])].keys():\n",
        "        # obtenemos su probabilidad y la sumamos al \"accumulator\"\n",
        "        accumulator += model[tuple(text[-3:])][word]\n",
        "        # si el \"accumulator\" es mayor a nuestro \"threshold\", añadimos la palabra al final del texto\n",
        "        if accumulator >= r:\n",
        "            text.append(word)\n",
        "            break\n",
        "\n",
        "    # Si las últimas dos palabras del texto es una secuencia de \"None\", se terminará el bucle\n",
        "    if text[-3:] == [None, None, None]:\n",
        "        sentence_finished = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "8PF0fI9t1Bjt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'the price is based on a reference price of 700 Ecus per tonne .'"
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\" \".join(text[:-3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
